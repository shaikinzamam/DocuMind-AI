ğŸ“˜ DocuMind AI â€” Intelligent Document Chat System

DocuMind AI is a local, AI-powered document assistant that allows users to upload PDF files and chat with their documents using a Retrieval-Augmented Generation (RAG) pipeline powered by Ollama and Streamlit.

It runs fully on your local machine, making it suitable for privacy-focused, offline, and low-cost AI document intelligence.

ğŸ§  What this project does

DocuMind AI enables you to:

Upload one or more PDF documents

Automatically extract and chunk text

Generate embeddings using a local model

Retrieve relevant content based on your question

Generate accurate answers grounded only in your documents

Display source-aware responses

Interact through a clean Streamlit web interface

ğŸ—ï¸ System Architecture (RAG Pipeline)
PDFs â†’ Text Extraction â†’ Chunking â†’ Embeddings (Ollama)
                         â†“
                    Vector Search
                         â†“
                   Relevant Chunks
                         â†“
                    Prompt Builder
                         â†“
                  Local LLM (Ollama)
                         â†“
                   Final Answer

âš™ï¸ Tech Stack

Python 3.10+

Streamlit â€“ web interface

Ollama â€“ local LLM & embeddings

Mistral â€“ response generation model

nomic-embed-text â€“ embedding model

PyPDF2 â€“ PDF text extraction

NumPy â€“ vector operations

âœ¨ Key Features

ğŸ“„ Multi-PDF upload support

ğŸ” Context-aware document retrieval

ğŸ§© Safe chunking to avoid context overflow

ğŸ¤– Fully local LLM (no paid APIs)

ğŸ“š Source-grounded answers

âš¡ Fast, lightweight Streamlit UI

ğŸ” Privacy-first design

ğŸ› ï¸ Installation & Setup
1ï¸âƒ£ Clone the repository
git clone https://github.com/shaikinzamam/DocuMind-AI.git
cd DocuMind-AI

2ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate   # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Install and run Ollama

Download Ollama from:
ğŸ‘‰ https://ollama.com

Then pull the required models:

ollama pull mistral
ollama pull nomic-embed-text


Make sure Ollama is running.

5ï¸âƒ£ Run the app
streamlit run app.py


Open in browser:
ğŸ‘‰ http://localhost:8501

ğŸ§ª Example Use Cases

Study assistant for PDFs

Research paper analysis

Notes & textbook chat

Legal / policy document search

Technical manual assistant

Resume & report understanding

ğŸ“Œ Project Highlights (Resume-ready)

Built a local Retrieval-Augmented Generation (RAG) system to enable document-grounded AI responses.

Designed a PDF ingestion pipeline with safe chunking and embedding error handling.

Integrated Ollama local LLMs for private, offline AI inference.

Developed a Streamlit-based intelligent document chat interface.

ğŸš§ Future Improvements

FAISS vector database integration

Page-level citation display

Chat memory & conversation summaries

OCR support for scanned PDFs

Document auto-summarization

Docker deployment

Multi-model support

ğŸ‘¨â€ğŸ’» Author

Shaik Inzamam
B.Tech CSE (AI/ML & CS)
Aspiring AI / LLM Engineer
